{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XsvCRudsQLx9"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"rdd_demo\").getOrCreate()\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"MiniTempratures\")\n",
        "sc = SparkContext.getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "Syk4pnkfQ-aK",
        "outputId": "0884c380-2931-45df-955c-e554bfc11eaf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc8caefd420>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d322ee8f8f42:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>rdd_demo</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015 = spark.read.option(\"inferSchema\",\"false\").option(\"header\",\"true\").csv(\"2015-summary.csv\")"
      ],
      "metadata": {
        "id": "dbR_N2xdRMVY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7HkF3aARncj",
        "outputId": "e250131a-d669-4d67-94cc-d35e14891608"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count='15'),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count='1'),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count='344'),\n",
              " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count='15'),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count='62')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFMYxtNwR8z5",
        "outputId": "de179f40-b0a9-4061-e515-2eac46960f6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: string]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(flightData2015)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "iqb1BebiR_2Q",
        "outputId": "409047a6-b9a1-4da9-991a-12d8bc99e3be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flightData2015.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UylblVXESCqv",
        "outputId": "cf2aac8e-bc09-4491-de66-2fa545c02fed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##read using datasource API\n",
        "flightData2015 = spark.read.option(\"inferSchema\",\"false\").option(\"header\",'true').csv(\"2015-summary.csv\")\n",
        "\n",
        "##commands\n",
        "flightData2015 = flightData2015.toDF(\"dest\",\"source\",\"count\").rdd\n",
        "print(type(flightData2015))\n",
        "print(flightData2015.take(1))\n",
        "print(flightData2015.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1cDDk6mSGvb",
        "outputId": "fa95f16c-65b2-48e3-cc9a-7d38dd6c89a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n",
            "[Row(dest='United States', source='Romania', count='15')]\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###read using sparkContext\n",
        "spth = \"2015-summary.csv\"\n",
        "sc_flightData2015 = spark.sparkContext.textFile(spth)\n",
        "print(type(sc_flightData2015))\n",
        "print(sc_flightData2015.take(2))\n",
        "print(sc_flightData2015.take(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoCEkiNfS-WD",
        "outputId": "030b6d45-87f6-4ea1-9035-ebd6e6e5aac1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n",
            "['DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count', 'United States,Romania,15']\n",
            "['DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count', 'United States,Romania,15', 'United States,Croatia,1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###convert pandas file to RDD\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "spth=\"2015-summary.csv\"\n",
        "pd_flightData2015 = pd.read_csv(spth,header = 0)\n",
        "print(type(pd_flightData2015))\n",
        "print(pd_flightData2015.head())\n",
        "pd_flightData2015 = spark.createDataFrame(pd_flightData2015).rdd\n",
        "print(type(pd_flightData2015))\n",
        "print(pd_flightData2015.take(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA1qAH2pTgJB",
        "outputId": "b3f3e69f-7df9-4d0f-d43a-6a003e14f5dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "  DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME  count\n",
            "0     United States             Romania     15\n",
            "1     United States             Croatia      1\n",
            "2     United States             Ireland    344\n",
            "3             Egypt       United States     15\n",
            "4     United States               India     62\n",
            "<class 'pyspark.rdd.RDD'>\n",
            "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
        "words = spark.sparkContext.parallelize(myCollection,2)\n",
        "print(type(words))\n",
        "print(words.collect())\n",
        "print(words.take(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAneXUDSUNGG",
        "outputId": "eee6cfa2-a530-456a-c469-dfc4debd15cf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n",
            "['Spark', 'The', 'Definitive', 'Guide', ':', 'Big', 'Data', 'Processing', 'Made', 'Simple']\n",
            "['Spark', 'The', 'Definitive', 'Guide', ':', 'Big', 'Data', 'Processing', 'Made', 'Simple']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###from collection of text\n",
        "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
        "words = spark.sparkContext.parallelize(myCollection,2)\n",
        "\n",
        "words.setName(\"myWords\")\n",
        "print(words.name())\n",
        "print(type(words))\n",
        "print(words.take(5))\n",
        "words.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTtsPYl6Ufh9",
        "outputId": "2a28fe0e-6cef-44dd-82d0-a56e34c0fccf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "myWords\n",
            "<class 'pyspark.rdd.RDD'>\n",
            "['Spark', 'The', 'Definitive', 'Guide', ':']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##from a range of numbers\n",
        "myRange = spark.range(1000000000000).toDF(\"numbers\").rdd.map(lambda row: row[0])\n",
        "\n",
        "print(myRange.take(10))\n",
        "print(type(myRange))\n",
        "myRange.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbiatvlJVKde",
        "outputId": "e599a2fe-62bb-4e92-eeaa-00ffc12ebfb6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "<class 'pyspark.rdd.PipelinedRDD'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###flitering\n",
        "\n",
        "def parseLine(line):\n",
        "  fields = line.split(',')\n",
        "  date = fields[0]\n",
        "  p_open = fields[1]\n",
        "  p_close = fields[5]\n",
        "  return (date,p_open,p_close)"
      ],
      "metadata": {
        "id": "HQx8U312V3FB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spth = \"RELIANCE.csv\"\n",
        "sdt = spark.sparkContext.textFile(spth)\n",
        "sdt =sdt.map(parseLine)\n",
        "sdt.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc1U3oG9WOsW",
        "outputId": "07fbdc51-649d-4640-88e8-71abdb9c1172"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Date', 'Open', 'Close'), ('2022-01-01', '100', '10000')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spth = \"RELIANCE.csv\"\n",
        "o_sdt = spark.read.format(\"CSV\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(spth)\n",
        "o_sdt = o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd.map(lambda row: (row[0], row[1], row[5]))\n",
        "print(o_sdt.count())\n",
        "print(type(o_sdt))\n",
        "print(o_sdt.take(5))\n",
        "\n",
        "o_sdt = o_sdt.filter(lambda row: row[2] > row[1])\n",
        "print(o_sdt.take(5))\n",
        "print(type(o_sdt))\n",
        "print(o_sdt.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oS1iylZWg0W",
        "outputId": "4b0ce5ab-5ade-452f-daa0-80c5f9fb8e38"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "<class 'pyspark.rdd.PipelinedRDD'>\n",
            "[(datetime.date(2022, 1, 1), 100, 10000), (datetime.date(2022, 1, 2), 200, 20000), (datetime.date(2022, 1, 3), 300, 30000), (datetime.date(2022, 1, 4), 400, 40000), (datetime.date(2022, 1, 5), 500, 50000)]\n",
            "[(datetime.date(2022, 1, 1), 100, 10000), (datetime.date(2022, 1, 2), 200, 20000), (datetime.date(2022, 1, 3), 300, 30000), (datetime.date(2022, 1, 4), 400, 40000), (datetime.date(2022, 1, 5), 500, 50000)]\n",
            "<class 'pyspark.rdd.PipelinedRDD'>\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##filter function\n",
        "\n",
        "def HighClose(row):\n",
        "  if(row[2] > row[1]):\n",
        "    return row"
      ],
      "metadata": {
        "id": "Ar1lDaOsW8O4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").load(spth)\n",
        "o_sdt = o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd.map(lambda row: (row[0], row[1], row[5]))\n",
        "print(o_sdt.count())\n",
        "o_sdt = o_sdt.filter(lambda row: HighClose(row))\n",
        "print(o_sdt.take(5))\n",
        "print(type(o_sdt))\n",
        "print(o_sdt.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NPZb0AcYUtC",
        "outputId": "828bb160-3307-4483-b5e8-4a41ca499c11"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "[('2022-01-01', '100', '10000'), ('2022-01-02', '200', '20000'), ('2022-01-03', '300', '30000'), ('2022-01-04', '400', '40000'), ('2022-01-05', '500', '50000')]\n",
            "<class 'pyspark.rdd.PipelinedRDD'>\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#maps\n",
        "def to_to_mill(row):\n",
        "    return (row[0], row[1], row[2], round(row[3],0))\n",
        ""
      ],
      "metadata": {
        "id": "uCnGMFPKZCQK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spth = \"RELIANCE.csv\"\n",
        "o_sdt = spark.read.format(\"CSV\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(spth)\n",
        "o_sdt = o_sdt.toDF(\"Date\",\"Open\",\"High\",\"Low\",\"Last\",\"Close\",\"Volume\",\"Turnover\").rdd.map(lambda row: (row[0], row[1], row[5], row[7]))\n",
        "print(o_sdt.take(2))\n",
        "o_sdt = o_sdt.map(to_to_mill)\n",
        "print(type(o_sdt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eVmCyvxiV1H",
        "outputId": "1c8367cc-6809-4641-f96e-118e30979986"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(datetime.date(2022, 1, 1), 100, 10000, None), (datetime.date(2022, 1, 2), 200, 20000, None)]\n",
            "<class 'pyspark.rdd.PipelinedRDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatmap\n",
        "def Func(lines):\n",
        "    lines = lines.lower()\n",
        "    lines = lines.split(\" \")\n",
        "    return lines\n",
        "\n",
        "#sc.stop()\n",
        "#conf = SparkConf().setMaster(\"local\").setAppName(\"wordcount\")\n",
        "#sc = SparkContext.getOrCreate()\n",
        "\n",
        "spth=\"sherlock_holmes.txt\"\n",
        "input_file = sc.textFile(spth)"
      ],
      "metadata": {
        "id": "hinx3f-ii3X1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdvcXNSqjRgl",
        "outputId": "a631f575-64e9-409c-9268-a67fd553bce8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\"]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = input_file.flatMap(Func)\n",
        "rdd1.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddXMbvDtjUm5",
        "outputId": "c53e06ab-2625-4cc1-b789-2f383d26ea2b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['project', \"gutenberg's\", 'the', 'adventures', 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1.map(lambda x: (x,1)).take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCHNPh_5j3iR",
        "outputId": "f644b80f-cb69-4868-fc73-d9646a1367ce"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('project', 1), (\"gutenberg's\", 1), ('the', 1), ('adventures', 1), ('of', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = rdd1.map(lambda x: (x,1)).groupByKey().mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)\n",
        "rdd2.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p9AxvAMj8XU",
        "outputId": "b8c8ad32-5330-4216-f31f-219f72cfb57d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(5704, 'the'), (3145, ''), (2882, 'and'), (2756, 'of'), (2719, 'to')]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##reduce\n",
        "spark.sparkContext.parallelize(range(1,5)).reduce(lambda x, y: x+y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I7YOq_kkMS1",
        "outputId": "d8a2b4b0-9716-4c52-cfa7-b0c6ba752142"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9JZ_4fWvkieh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}